import{_ as r,c as d,J as a,m as i,a as s,w as t,V as h,E as e,o}from"./chunks/framework.afRX3mrX.js";const P=JSON.parse('{"title":"Tokenize, Vectorize and Embedding","description":"","frontmatter":{"tags":["LLM","tokenize","vectorize","embedding"]},"headers":[],"relativePath":"Notes/Road to LLM/Tokenize, Vectorize and Embedding.md","filePath":"Notes/Road to LLM/Tokenize, Vectorize and Embedding.md"}'),g={name:"Notes/Road to LLM/Tokenize, Vectorize and Embedding.md"},B=i("h1",{id:"tokenize-vectorize-and-embedding",tabindex:"-1"},[s("Tokenize, Vectorize and Embedding "),i("a",{class:"header-anchor",href:"#tokenize-vectorize-and-embedding","aria-label":'Permalink to "Tokenize, Vectorize and Embedding"'},"​")],-1),y=h("",3),F=i("ol",null,[i("li",null,[s("Load model. (So the tokenizer is different from each model, in this case, "),i("code",null,"llama"),s(" use "),i("code",null,"SentencePiece"),s(" from Google (Not from llama itself))")]),i("li",null,[i("code",null,"SentencePiece"),s(": "),i("strong",null,"A versatile subword tokenizer and detokenizer for neural text processing"),s(". SentencePiece is a language-independent subword tokenizer and detokenizer designed for neural text processing, including neural machine translation (NMT)")])],-1),c=i("li",null,"Encode a string into a list of token IDs.",-1),A=i("h2",{id:"cost",tabindex:"-1"},[s("Cost "),i("a",{class:"header-anchor",href:"#cost","aria-label":'Permalink to "Cost"'},"​")],-1),D=i("p",null,[s("Try to run "),i("code",null,"tokenizer"),s(" for "),i("code",null,"meta-llama/llama")],-1),C=h("",15),f=i("li",null,null,-1),u=i("h2",{id:"contributor",tabindex:"-1"},[s("Contributor "),i("a",{class:"header-anchor",href:"#contributor","aria-label":'Permalink to "Contributor"'},"​")],-1),E=i("h2",{id:"changelog",tabindex:"-1"},[s("Changelog "),i("a",{class:"header-anchor",href:"#changelog","aria-label":'Permalink to "Changelog"'},"​")],-1);function m(b,_,v,w,z,q){const l=e("NolebasePageProperties"),n=e("VPNolebaseInlineLinkPreview"),k=e("NolebaseGitContributors"),p=e("NolebaseGitChangelog");return o(),d("div",null,[B,a(l),y,i("ol",null,[i("li",null,[s("Read "),a(n,{href:"https://github.com/meta-llama/llama/blob/54c22c0d63a3f3c9e77f43a6a3041c00018f4964/llama/tokenizer.py#L14",target:"_blank",rel:"noreferrer"},{default:t(()=>[s("tokenizer.py")]),_:1}),F]),c]),A,D,i("p",null,[s("My code is (modified from "),a(n,{href:"https://gist.github.com/slaren/9f26fc4cb24685d42601b1d91d70a13a",target:"_blank",rel:"noreferrer"},{default:t(()=>[s("here")]),_:1}),s(")")]),C,i("ul",null,[i("li",null,[a(n,{href:"https://thenewstack.io/the-building-blocks-of-llms-vectors-tokens-and-embeddings/#:~:text=Embeddings%20are%20what%20allow%20LLMs,its%20relationships%20with%20other%20tokens",target:"_blank",rel:"noreferrer"},{default:t(()=>[s("https://thenewstack.io/the-building-blocks-of-llms-vectors-tokens-and-embeddings/#:~:text=Embeddings are what allow LLMs,its relationships with other tokens")]),_:1}),s(".")]),i("li",null,[a(n,{href:"https://medium.com/@liusimao8/using-llama-2-models-for-text-embedding-with-langchain-79183350593d",target:"_blank",rel:"noreferrer"},{default:t(()=>[s("https://medium.com/@liusimao8/using-llama-2-models-for-text-embedding-with-langchain-79183350593d")]),_:1})]),f]),u,a(k),E,a(p)])}const x=r(g,[["render",m]]);export{P as __pageData,x as default};
