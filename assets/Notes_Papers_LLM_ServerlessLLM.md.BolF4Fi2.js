import{_ as c,c as h,m as a,a as e,J as r,w as i,V as d,E as t,o as p}from"./chunks/framework.afRX3mrX.js";const S=JSON.parse('{"title":"ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models paper repo","description":"","frontmatter":{"tags":["Papers","LLM","Serverless"]},"headers":[],"relativePath":"Notes/Papers/LLM/ServerlessLLM.md","filePath":"Notes/Papers/LLM/ServerlessLLM.md"}'),u={name:"Notes/Papers/LLM/ServerlessLLM.md"},g={id:"serverlessllm-locality-enhanced-serverless-inference-for-large-language-models-paper-repo",tabindex:"-1"},m=a("a",{class:"header-anchor",href:"#serverlessllm-locality-enhanced-serverless-inference-for-large-language-models-paper-repo","aria-label":'Permalink to "ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models [paper](https://arxiv.org/abs/2401.14351) [repo](https://github.com/ServerlessLLM/ServerlessLLM)"'},"​",-1),f=d('<h2 id="what-are-the-motivations-for-this-work" tabindex="-1">What are the <em>motivations</em> for this work? <a class="header-anchor" href="#what-are-the-motivations-for-this-work" aria-label="Permalink to &quot;What are the _motivations_ for this work?&quot;">​</a></h2><p>Large Language Models (LLMs) have recently been incorporated into various online applications, and Serving LLM inference at scale is a challenging problem. LLM requires huge GPU resources, which is expensive today.</p><p>The technical challenge of serving LLM includes:</p><ol><li>LLM is a memory intensive application</li><li>highly dynamic, bursty traffic for LLM application</li></ol><p>The technical challenge of serving LLM as Microservices includes:</p><ol><li>Transport from model repositories</li><li>Costly checkpoint loading from storage devices</li></ol><p>The current solution for Machine Learning Serverless Application include checkpointing, which has significant overheads and latencies. And there are other solutions (and drawbacks) for Regular Serverless Application:</p><ol><li>Keep instances warm: waste GPU resources</li><li>Memory Caching: not efficient for Large Models</li><li>Additional Storage Server: Large communication overhead</li></ol><h2 id="what-is-the-proposed-solution" tabindex="-1"><strong>What is the proposed <em>solution</em>?</strong> <a class="header-anchor" href="#what-is-the-proposed-solution" aria-label="Permalink to &quot;**What is the proposed _solution_?**&quot;">​</a></h2><p>Leveraging the multi-tier storage architecture for local checkpoint storage and utilizing their significant storage bandwidth for efficient checkpoint loading, which includes:</p><ol><li>Fast LLM checkpoint loading: Increase memory addressing efficiency</li><li>Locality-driven LLM inference with <strong>live migration</strong></li><li>Locality-aware server allocation</li></ol><h3 id="fast-llm-checkpoint-loading" tabindex="-1">Fast LLM checkpoint loading <a class="header-anchor" href="#fast-llm-checkpoint-loading" aria-label="Permalink to &quot;Fast LLM checkpoint loading&quot;">​</a></h3><p><img src="https://webresources.aaaab3n.moe/uPic/x9zbVr.png" alt="x9zbVr"></p><ul><li>Preloading on GPU</li><li>Utilize parallelized PCIE, direct r/w and throughput optimization.</li><li><strong>Question Mark</strong>: Preloading may introduce new overheads, and description here is not clear enough.</li></ul><h3 id="locality-driven-llm-inference-with-live-migration" tabindex="-1">Locality-Driven LLM Inference with Live Migration <a class="header-anchor" href="#locality-driven-llm-inference-with-live-migration" aria-label="Permalink to &quot;Locality-Driven LLM Inference with Live Migration&quot;">​</a></h3><p><img src="https://webresources.aaaab3n.moe/uPic/IBRwpi.png" alt="IBRwpi"><img src="https://webresources.aaaab3n.moe/uPic/uyNbYu.png" alt="uyNbYu"></p><ul><li>token-based migration</li><li>And fault tolerance</li></ul><ol><li>The model loading scheduler sends a model loading request to dest server to load model A into GPUs. If there is an idle instance of model A on dest server, the scheduler skips this step.</li><li>After loading, the scheduler sends a migration request carrying the address of dest server to src server.</li><li>Upon receiving a migrate request, src server sets itself as “migrating”, sends a resume request with intermediate tokens (i.e., input tokens and the output tokens produced before step 3) to dest server if the inference is not completed. Otherwise, it immediately returns to the scheduler.</li><li>dest server recomputes KV cache given the tokens in the resume request.</li><li>Once resume request is done, src server stops inference,returns to the scheduler, and replies to the request router with all tokens (i.e., the intermediate tokens together with the remaining tokens produced between step 3 and step 5) and a flag “migrated”. If long-context, the collection of all tokens can be very large thus resuming takes a long time, during which many new tokens are predicted. In such a case, we can repeat the above two steps to further reduce the tokens to send between src and dest.</li><li>The scheduler finishes the migration, unloads model A at src server and starts loading model B.</li><li>The request router checks the flag in the inference response. If it is “migrated”, the request router replaces src server with dest server in its route table and sends all tokens to dest server to continue inference.</li></ol><h3 id="locality-aware-server-allocation" tabindex="-1">Locality-Aware Server Allocation <a class="header-anchor" href="#locality-aware-server-allocation" aria-label="Permalink to &quot;Locality-Aware Server Allocation&quot;">​</a></h3><p><img src="https://webresources.aaaab3n.moe/uPic/Rl6i2d.png" alt="Rl6i2d"></p><h2 id="what-is-the-work-s-evaluation-of-the-proposed-solution" tabindex="-1"><strong>What is the work&#39;s <em>evaluation</em> of the proposed solution?</strong> <a class="header-anchor" href="#what-is-the-work-s-evaluation-of-the-proposed-solution" aria-label="Permalink to &quot;**What is the work&#39;s _evaluation_ of the proposed solution?**&quot;">​</a></h2><p>ServerlessLLM demonstrated a 10-200X improvement in latency for running OPT model inferences across datasets, which shows ServerlessLLM’s effectiveness.</p><h2 id="contributor" tabindex="-1">Contributor <a class="header-anchor" href="#contributor" aria-label="Permalink to &quot;Contributor&quot;">​</a></h2>',23),v=a("h2",{id:"changelog",tabindex:"-1"},[e("Changelog "),a("a",{class:"header-anchor",href:"#changelog","aria-label":'Permalink to "Changelog"'},"​")],-1);function L(b,w,k,_,M,P){const o=t("VPNolebaseInlineLinkPreview"),s=t("NolebasePageProperties"),n=t("NolebaseGitContributors"),l=t("NolebaseGitChangelog");return p(),h("div",null,[a("h1",g,[e("ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models "),r(o,{href:"https://arxiv.org/abs/2401.14351",target:"_blank",rel:"noreferrer"},{default:i(()=>[e("paper")]),_:1}),e(),r(o,{href:"https://github.com/ServerlessLLM/ServerlessLLM",target:"_blank",rel:"noreferrer"},{default:i(()=>[e("repo")]),_:1}),e(),m]),r(s),f,r(n),v,r(l)])}const q=c(u,[["render",L]]);export{S as __pageData,q as default};
