import{_ as r,c as d,J as a,m as s,a as i,w as t,V as h,E as e,o}from"./chunks/framework.afRX3mrX.js";const T=JSON.parse('{"title":"Tokenize, Vectorize and Embedding","description":"","frontmatter":{"tags":["LLM","tokenize","vectorize","embedding"]},"headers":[],"relativePath":"Notes/Research/Road to LLM/Tokenize, Vectorize and Embedding.md","filePath":"Notes/Research/Road to LLM/Tokenize, Vectorize and Embedding.md"}'),g={name:"Notes/Research/Road to LLM/Tokenize, Vectorize and Embedding.md"},B=s("h1",{id:"tokenize-vectorize-and-embedding",tabindex:"-1"},[i("Tokenize, Vectorize and Embedding "),s("a",{class:"header-anchor",href:"#tokenize-vectorize-and-embedding","aria-label":'Permalink to "Tokenize, Vectorize and Embedding"'},"​")],-1),y=h("",3),F=s("ol",null,[s("li",null,[i("Load model. (So the tokenizer is different from each model, in this case, "),s("code",null,"llama"),i(" use "),s("code",null,"SentencePiece"),i(" from Google (Not from llama itself))")]),s("li",null,[s("code",null,"SentencePiece"),i(": "),s("strong",null,"A versatile subword tokenizer and detokenizer for neural text processing"),i(". SentencePiece is a language-independent subword tokenizer and detokenizer designed for neural text processing, including neural machine translation (NMT)")])],-1),c=s("li",null,"Encode a string into a list of token IDs.",-1),A=s("h2",{id:"cost",tabindex:"-1"},[i("Cost "),s("a",{class:"header-anchor",href:"#cost","aria-label":'Permalink to "Cost"'},"​")],-1),D=s("p",null,[i("Try to run "),s("code",null,"tokenizer"),i(" for "),s("code",null,"meta-llama/llama")],-1),C=h("",16),f=s("li",null,null,-1),u=s("h2",{id:"contributor",tabindex:"-1"},[i("Contributor "),s("a",{class:"header-anchor",href:"#contributor","aria-label":'Permalink to "Contributor"'},"​")],-1),E=s("h2",{id:"changelog",tabindex:"-1"},[i("Changelog "),s("a",{class:"header-anchor",href:"#changelog","aria-label":'Permalink to "Changelog"'},"​")],-1);function m(b,v,_,w,z,q){const l=e("NolebasePageProperties"),n=e("VPNolebaseInlineLinkPreview"),k=e("NolebaseGitContributors"),p=e("NolebaseGitChangelog");return o(),d("div",null,[B,a(l),y,s("ol",null,[s("li",null,[i("Read "),a(n,{href:"https://github.com/meta-llama/llama/blob/54c22c0d63a3f3c9e77f43a6a3041c00018f4964/llama/tokenizer.py#L14",target:"_blank",rel:"noreferrer"},{default:t(()=>[i("tokenizer.py")]),_:1}),F]),c]),A,D,s("p",null,[i("My code is (modified from "),a(n,{href:"https://gist.github.com/slaren/9f26fc4cb24685d42601b1d91d70a13a",target:"_blank",rel:"noreferrer"},{default:t(()=>[i("here")]),_:1}),i(")")]),C,s("ul",null,[s("li",null,[a(n,{href:"https://thenewstack.io/the-building-blocks-of-llms-vectors-tokens-and-embeddings/#:~:text=Embeddings%20are%20what%20allow%20LLMs,its%20relationships%20with%20other%20tokens",target:"_blank",rel:"noreferrer"},{default:t(()=>[i("https://thenewstack.io/the-building-blocks-of-llms-vectors-tokens-and-embeddings/#:~:text=Embeddings are what allow LLMs,its relationships with other tokens")]),_:1}),i(".")]),s("li",null,[a(n,{href:"https://medium.com/@liusimao8/using-llama-2-models-for-text-embedding-with-langchain-79183350593d",target:"_blank",rel:"noreferrer"},{default:t(()=>[i("https://medium.com/@liusimao8/using-llama-2-models-for-text-embedding-with-langchain-79183350593d")]),_:1})]),f]),u,a(k),E,a(p)])}const x=r(g,[["render",m]]);export{T as __pageData,x as default};
