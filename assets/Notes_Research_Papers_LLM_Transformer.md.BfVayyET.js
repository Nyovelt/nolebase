import{_ as m,c as s,J as a,m as e,a as t,w as r,V as l,E as n,o as i}from"./chunks/framework.afRX3mrX.js";const z=JSON.parse('{"title":"Transformer","description":"","frontmatter":{"tags":["Papers","PhD","Transformer","LLM"]},"headers":[],"relativePath":"Notes/Research/Papers/LLM/Transformer.md","filePath":"Notes/Research/Papers/LLM/Transformer.md"}'),u={name:"Notes/Research/Papers/LLM/Transformer.md"},p=e("h1",{id:"transformer",tabindex:"-1"},[t("Transformer "),e("a",{class:"header-anchor",href:"#transformer","aria-label":'Permalink to "Transformer"'},"​")],-1),f=l('<h2 id="what-are-the-motivations-for-this-work" tabindex="-1"><strong>What are the <em>motivations</em> for this work?</strong> <a class="header-anchor" href="#what-are-the-motivations-for-this-work" aria-label="Permalink to &quot;**What are the _motivations_ for this work?**&quot;">​</a></h2><p>Recurrent Neural Network is the State-Of-the-Art approaches in NLP. And people want to improve its model quality and training efficiency (use more GPU for example).</p><p>The fundamental constraint of sequential computation <strong>limit its upper bound of performance</strong>.</p><h2 id="what-is-the-proposed-solution" tabindex="-1"><strong>What is the proposed <em>solution</em>?</strong> <a class="header-anchor" href="#what-is-the-proposed-solution" aria-label="Permalink to &quot;**What is the proposed _solution_?**&quot;">​</a></h2><p>Transformer eschewed recurrence (the part of sequential computation) and relied on <strong>attention mechanism</strong>, which allowed for significantly more parallelization.</p><h3 id="multi-head-attention" tabindex="-1">Multi-head Attention <a class="header-anchor" href="#multi-head-attention" aria-label="Permalink to &quot;Multi-head Attention&quot;">​</a></h3>',6),_=e("img",{src:"https://webresources.aaaab3n.moe/share/SCR-20240322-ndvv.png",alt:""},null,-1),Q={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},T={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.666ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.89ex",height:"1.663ex",role:"img",focusable:"false",viewBox:"0 -441 1277.4 735.2","aria-hidden":"true"},g=l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(562,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(529,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width:3;"></path></g></g></g></g></g>',1),w=[g],b=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("msub",null,[e("mi",null,"a"),e("mrow",{"data-mjx-texclass":"ORD"},[e("mi",null,"a"),e("mi",null,"j")])])])],-1),k=e("h3",{id:"position-wise-feed-forward-networks",tabindex:"-1"},[t("Position-wise Feed-Forward Networks "),e("a",{class:"header-anchor",href:"#position-wise-feed-forward-networks","aria-label":'Permalink to "Position-wise Feed-Forward Networks"'},"​")],-1),x=e("p",null,"each of the layers in our encoder and decoder contains a fully connected feed-forward network",-1),P=e("h2",{id:"related-knowledge",tabindex:"-1"},[t("Related Knowledge "),e("a",{class:"header-anchor",href:"#related-knowledge","aria-label":'Permalink to "Related Knowledge"'},"​")],-1),N=e("li",null,"Hidden State",-1),y=e("li",null,[t("Encoder-Decoder architecture "),e("ul",null,[e("li",null,[e("img",{src:"https://webresources.aaaab3n.moe/share/SCR-20240322-chfl.png",alt:""})])])],-1),v=e("li",null,[t("Self-attention "),e("ul",null,[e("li",null,"Attention, broadly construed, is a method for taking a query, and softly looking up information in a key-value store by picking the value(s) of the key(s) most like the query.")])],-1),L=e("h2",{id:"useful-links",tabindex:"-1"},[t("Useful Links "),e("a",{class:"header-anchor",href:"#useful-links","aria-label":'Permalink to "Useful Links"'},"​")],-1),M=e("h2",{id:"contributor",tabindex:"-1"},[t("Contributor "),e("a",{class:"header-anchor",href:"#contributor","aria-label":'Permalink to "Contributor"'},"​")],-1),C=e("h2",{id:"changelog",tabindex:"-1"},[t("Changelog "),e("a",{class:"header-anchor",href:"#changelog","aria-label":'Permalink to "Changelog"'},"​")],-1);function R(S,V,A,q,j,D){const h=n("NolebasePageProperties"),o=n("VPNolebaseInlineLinkPreview"),d=n("NolebaseGitContributors"),c=n("NolebaseGitChangelog");return i(),s("div",null,[p,a(h),f,e("p",null,[t("each of the layers in our encoder and decoder contains a fully connected feed-forward network "),_,t(" Lora is applies to "),e("mjx-container",Q,[(i(),s("svg",T,w)),b])]),k,x,P,e("ul",null,[e("li",null,[a(o,{href:"https://en.wikipedia.org/wiki/Recurrent_neural_network",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("RNN")]),_:1}),e("ul",null,[e("li",null,[a(o,{href:"https://en.wikipedia.org/wiki/Long_short-term_memory",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("Long short-term memory (LSTM)")]),_:1})]),N])]),y,v]),L,e("p",null,[a(o,{href:"https://youtu.be/P127jhj-8-Y?si=ORJMt5Mam7pNxCQN",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("https://youtu.be/P127jhj-8-Y?si=ORJMt5Mam7pNxCQN")]),_:1}),a(o,{href:"https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf")]),_:1}),a(o,{href:"https://zhuanlan.zhihu.com/p/604739354",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("https://zhuanlan.zhihu.com/p/604739354")]),_:1})]),M,a(d),C,a(c)])}const G=m(u,[["render",R]]);export{z as __pageData,G as default};
