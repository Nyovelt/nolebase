import{_ as m,c as s,J as a,m as e,a as t,w as r,V as l,E as n,o as i}from"./chunks/framework.afRX3mrX.js";const z=JSON.parse('{"title":"Transformer","description":"","frontmatter":{"tags":["Papers","PhD","Transformer","LLM"]},"headers":[],"relativePath":"Notes/Papers/LLM/Transformer.md","filePath":"Notes/Papers/LLM/Transformer.md"}'),u={name:"Notes/Papers/LLM/Transformer.md"},p=e("h1",{id:"transformer",tabindex:"-1"},[t("Transformer "),e("a",{class:"header-anchor",href:"#transformer","aria-label":'Permalink to "Transformer"'},"​")],-1),f=l("",6),_=e("img",{src:"https://webresources.aaaab3n.moe/share/SCR-20240322-ndvv.png",alt:""},null,-1),Q={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},T={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.666ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.89ex",height:"1.663ex",role:"img",focusable:"false",viewBox:"0 -441 1277.4 735.2","aria-hidden":"true"},g=l("",1),w=[g],b=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("msub",null,[e("mi",null,"a"),e("mrow",{"data-mjx-texclass":"ORD"},[e("mi",null,"a"),e("mi",null,"j")])])])],-1),k=e("h3",{id:"position-wise-feed-forward-networks",tabindex:"-1"},[t("Position-wise Feed-Forward Networks "),e("a",{class:"header-anchor",href:"#position-wise-feed-forward-networks","aria-label":'Permalink to "Position-wise Feed-Forward Networks"'},"​")],-1),x=e("p",null,"each of the layers in our encoder and decoder contains a fully connected feed-forward network",-1),P=e("h2",{id:"related-knowledge",tabindex:"-1"},[t("Related Knowledge "),e("a",{class:"header-anchor",href:"#related-knowledge","aria-label":'Permalink to "Related Knowledge"'},"​")],-1),N=e("li",null,"Hidden State",-1),y=e("li",null,[t("Encoder-Decoder architecture "),e("ul",null,[e("li",null,[e("img",{src:"https://webresources.aaaab3n.moe/share/SCR-20240322-chfl.png",alt:""})])])],-1),v=e("li",null,[t("Self-attention "),e("ul",null,[e("li",null,"Attention, broadly construed, is a method for taking a query, and softly looking up information in a key-value store by picking the value(s) of the key(s) most like the query.")])],-1),L=e("h2",{id:"useful-links",tabindex:"-1"},[t("Useful Links "),e("a",{class:"header-anchor",href:"#useful-links","aria-label":'Permalink to "Useful Links"'},"​")],-1),M=e("h2",{id:"contributor",tabindex:"-1"},[t("Contributor "),e("a",{class:"header-anchor",href:"#contributor","aria-label":'Permalink to "Contributor"'},"​")],-1),C=e("h2",{id:"changelog",tabindex:"-1"},[t("Changelog "),e("a",{class:"header-anchor",href:"#changelog","aria-label":'Permalink to "Changelog"'},"​")],-1);function S(R,V,A,q,j,D){const d=n("NolebasePageProperties"),o=n("VPNolebaseInlineLinkPreview"),h=n("NolebaseGitContributors"),c=n("NolebaseGitChangelog");return i(),s("div",null,[p,a(d),f,e("p",null,[t("each of the layers in our encoder and decoder contains a fully connected feed-forward network "),_,t(" Lora is applies to "),e("mjx-container",Q,[(i(),s("svg",T,w)),b])]),k,x,P,e("ul",null,[e("li",null,[a(o,{href:"https://en.wikipedia.org/wiki/Recurrent_neural_network",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("RNN")]),_:1}),e("ul",null,[e("li",null,[a(o,{href:"https://en.wikipedia.org/wiki/Long_short-term_memory",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("Long short-term memory (LSTM)")]),_:1})]),N])]),y,v]),L,e("p",null,[a(o,{href:"https://youtu.be/P127jhj-8-Y?si=ORJMt5Mam7pNxCQN",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("https://youtu.be/P127jhj-8-Y?si=ORJMt5Mam7pNxCQN")]),_:1}),a(o,{href:"https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf")]),_:1}),a(o,{href:"https://zhuanlan.zhihu.com/p/604739354",target:"_blank",rel:"noreferrer"},{default:r(()=>[t("https://zhuanlan.zhihu.com/p/604739354")]),_:1})]),M,a(h),C,a(c)])}const G=m(u,[["render",S]]);export{z as __pageData,G as default};
