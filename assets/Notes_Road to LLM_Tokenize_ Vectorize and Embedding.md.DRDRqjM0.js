import{_ as r,c as p,J as a,m as s,a as i,w as t,V as l,E as e,o as d}from"./chunks/framework.afRX3mrX.js";const q=JSON.parse('{"title":"Tokenize, Vectorize and Embedding","description":"","frontmatter":{"tags":["LLM","tokenize","vectorize","embedding"]},"headers":[],"relativePath":"Notes/Road to LLM/Tokenize, Vectorize and Embedding.md","filePath":"Notes/Road to LLM/Tokenize, Vectorize and Embedding.md"}'),c={name:"Notes/Road to LLM/Tokenize, Vectorize and Embedding.md"},g=s("h1",{id:"tokenize-vectorize-and-embedding",tabindex:"-1"},[i("Tokenize, Vectorize and Embedding "),s("a",{class:"header-anchor",href:"#tokenize-vectorize-and-embedding","aria-label":'Permalink to "Tokenize, Vectorize and Embedding"'},"​")],-1),B=l('<h2 id="what-is-token" tabindex="-1">What is token <a class="header-anchor" href="#what-is-token" aria-label="Permalink to &quot;What is token&quot;">​</a></h2><p><img src="https://webresources.aaaab3n.moe/uPic/NvdeuG.png" alt="NvdeuG"> Token is more like translate human language to llm readable language use &quot;vocabulary&quot;. In compilers, &quot;<strong>Lexical tokenization</strong> is conversion of a text into (semantically or syntactically) meaningful <em>lexical tokens</em> belonging to categories defined by a &quot;lexer&quot; program&quot;, similar in LLM, LLM will <em>tokeninze</em> word into a &quot;number&quot; mostly depend on its &quot;word class&quot; or language properties.</p><h2 id="how-to-generate-token" tabindex="-1">How to generate token <a class="header-anchor" href="#how-to-generate-token" aria-label="Permalink to &quot;How to generate token&quot;">​</a></h2>',3),y=s("ol",null,[s("li",null,[i("Load model. (So the tokenizer is different from each model, in this case, "),s("code",null,"llama"),i(" use "),s("code",null,"SentencePiece"),i(" from Google (Not from llama itself))")])],-1),m=s("li",null,"Encode a string into a list of token IDs.",-1),u=s("h2",{id:"cost",tabindex:"-1"},[i("Cost "),s("a",{class:"header-anchor",href:"#cost","aria-label":'Permalink to "Cost"'},"​")],-1),_=s("p",null,[i("Try to run "),s("code",null,"tokenizer"),i(" for "),s("code",null,"meta-llama/llama")],-1),C=l(`<div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> llama </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> Tokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">model_path </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;"> &#39;tokenizer.model&#39;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;"> Tokenizer</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">model_path</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">model_path)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">text </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;"> &quot;What is the answer to the life, the universe and everthing?&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tokens </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">encode</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(text, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">bos</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#E06C75;--shiki-light-font-style:inherit;--shiki-dark-font-style:italic;">eos</span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(tokens)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> tid </span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> tokens:</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tk </span><span style="--shiki-light:#D73A49;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;"> tokenizer.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">decode</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">([tid])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">f</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tid</span><span style="--shiki-light:#D73A49;--shiki-dark:#C678DD;">:&gt;6</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;"> -&gt; &#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">tk</span><span style="--shiki-light:#005CC5;--shiki-dark:#D19A66;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#98C379;">&#39;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#56B6C2;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(tokenizer.</span><span style="--shiki-light:#24292E;--shiki-dark:#61AFEF;">decode</span><span style="--shiki-light:#24292E;--shiki-dark:#ABB2BF;">(tokens))</span></span></code></pre></div><p>It runs on cpu, cost 200MB memory and 20% of 16 core CPU (paralleled).</p><h2 id="what-is-vectorize" tabindex="-1">What is Vectorize <a class="header-anchor" href="#what-is-vectorize" aria-label="Permalink to &quot;What is Vectorize&quot;">​</a></h2><h2 id="contributor" tabindex="-1">Contributor <a class="header-anchor" href="#contributor" aria-label="Permalink to &quot;Contributor&quot;">​</a></h2>`,4),b=s("h2",{id:"changelog",tabindex:"-1"},[i("Changelog "),s("a",{class:"header-anchor",href:"#changelog","aria-label":'Permalink to "Changelog"'},"​")],-1);function A(f,E,F,z,D,T){const o=e("NolebasePageProperties"),n=e("VPNolebaseInlineLinkPreview"),h=e("NolebaseGitContributors"),k=e("NolebaseGitChangelog");return d(),p("div",null,[g,a(o),B,s("ol",null,[s("li",null,[i("Read "),a(n,{href:"https://github.com/meta-llama/llama/blob/54c22c0d63a3f3c9e77f43a6a3041c00018f4964/llama/tokenizer.py#L14",target:"_blank",rel:"noreferrer"},{default:t(()=>[i("tokenizer.py")]),_:1}),y]),m]),u,_,s("p",null,[i("My code is (modified from "),a(n,{href:"https://gist.github.com/slaren/9f26fc4cb24685d42601b1d91d70a13a",target:"_blank",rel:"noreferrer"},{default:t(()=>[i("here")]),_:1}),i(")")]),C,a(h),b,a(k)])}const v=r(c,[["render",A]]);export{q as __pageData,v as default};
